{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Show a 2D plot with the data in beat\n",
    "def display_signal(beat):\n",
    "    plt.plot(beat)\n",
    "    plt.ylabel('Signal')\n",
    "    plt.show()\n",
    "# Class for RR intervals features\n",
    "class RR_intervals:\n",
    "    def __init__(self):\n",
    "        # Instance atributes\n",
    "        self.pre_R = np.array([])\n",
    "        self.post_R = np.array([])\n",
    "        self.local_R = np.array([])\n",
    "        self.global_R = np.array([])\n",
    "class mit_db:\n",
    "    def __init__(self):\n",
    "        # Instance attributes\n",
    "        self.filename = []\n",
    "        self.raw_signal = []\n",
    "        self.beat = np.empty([])  # record, beat, lead\n",
    "        self.class_ID = []\n",
    "        self.valid_R = []\n",
    "        self.R_pos = []\n",
    "        self.orig_R_pos = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_labels_name(DS, winL, winR, do_preprocess, maxRR, use_RR, norm_RR, compute_morph, db_path,\n",
    "                                reduced_DS, leads_flag):\n",
    "    \"\"\"\n",
    "\n",
    "    :param DS:\n",
    "    :param winL:\n",
    "    :param winR:\n",
    "    :param do_preprocess:\n",
    "    :param maxRR:\n",
    "    :param use_RR:\n",
    "    :param norm_RR:\n",
    "    :param compute_morph:\n",
    "    :param db_path:\n",
    "    :param reduced_DS:\n",
    "    :param leads_flag:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    features_labels_name = db_path + '/features/' + 'w_' + str(winL) + '_' + str(winR) + '_' + DS\n",
    "\n",
    "    if do_preprocess:\n",
    "        features_labels_name += '_rm_bsline'\n",
    "\n",
    "    if maxRR:\n",
    "        features_labels_name += '_maxRR'\n",
    "\n",
    "    if use_RR:\n",
    "        features_labels_name += '_RR'\n",
    "\n",
    "    if norm_RR:\n",
    "        features_labels_name += '_norm_RR'\n",
    "\n",
    "    for descp in compute_morph:\n",
    "        features_labels_name += '_' + descp\n",
    "\n",
    "    if reduced_DS:\n",
    "        features_labels_name += '_reduced'\n",
    "\n",
    "    if leads_flag[0] == 1:\n",
    "        features_labels_name += '_MLII'\n",
    "\n",
    "    if leads_flag[1] == 1:\n",
    "        features_labels_name += '_V1'\n",
    "\n",
    "    features_labels_name += '.p'\n",
    "\n",
    "    return features_labels_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signal(DS, winL, winR, do_preprocess):\n",
    "    class_ID = [[] for i in range(len(DS))]\n",
    "    beat = [[] for i in range(len(DS))]  # record, beat, lead\n",
    "    R_poses = [np.array([]) for i in range(len(DS))]\n",
    "    Original_R_poses = [np.array([]) for i in range(len(DS))]\n",
    "    valid_R = [np.array([]) for i in range(len(DS))]\n",
    "    my_db = mit_db()\n",
    "\n",
    "    size_RR_max = 20\n",
    "\n",
    "    pathDB = os.getcwd()\n",
    "    print pathDB\n",
    "    DB_name = 'data'\n",
    "\n",
    "    # Read files: signal (.csv )  annotations (.txt)\n",
    "    fRecords = list()\n",
    "    fAnnotations = list()\n",
    "\n",
    "    lst = os.listdir(pathDB + \"/\" + DB_name + \"/csv\")\n",
    "    lst.sort()\n",
    "    for filename in lst:\n",
    "        if filename.endswith(\".csv\"):\n",
    "            if int(filename[0:3]) in DS:\n",
    "                fRecords.append(filename)\n",
    "        elif filename.endswith(\".txt\"):\n",
    "            if int(filename[0:3]) in DS:\n",
    "                fAnnotations.append(filename)\n",
    "\n",
    "    MITBIH_classes = ['N', 'L', 'R', 'e', 'j', 'A', 'a', 'J', 'S', 'V', 'E', 'F', 'P', '/', 'f', 'u']\n",
    "    AAMI_classes = [['N', 'L', 'R'], ['A', 'a', 'J', 'S', 'e', 'j'], ['V', 'E'], ['F'], ['P', '/', 'f', 'u']]\n",
    "\n",
    "    RAW_signals = []\n",
    "\n",
    "    # for r, a in zip(fRecords, fAnnotations):\n",
    "    for r in range(0, len(fRecords)):\n",
    "\n",
    "        print 'Processing signal ' + str(r) + ' / ' + str(len(fRecords)) + '...'\n",
    "\n",
    "        filename = pathDB + \"/\" + DB_name + \"/csv/\" + fRecords[r]\n",
    "        print filename\n",
    "        f = open(filename, 'rb')\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        next(reader)  # skip first line!\n",
    "        MLII_index = 1\n",
    "        V1_index = 2\n",
    "        if int(fRecords[r][0:3]) == 114:\n",
    "            MLII_index = 2\n",
    "            V1_index = 1\n",
    "\n",
    "        MLII = []\n",
    "        V1 = []\n",
    "        for row in reader:\n",
    "            MLII.append((int(row[MLII_index])))\n",
    "            V1.append((int(row[V1_index])))\n",
    "        f.close()\n",
    "\n",
    "        RAW_signals.append((MLII, V1))  ## NOTE a copy must be created in order to preserve the original signal\n",
    "        # display_signal(MLII)\n",
    "\n",
    "        # 2. Read annotations\n",
    "        filename = pathDB + \"/\" + DB_name + \"/csv/\" + fAnnotations[r]\n",
    "        print filename\n",
    "        f = open(filename, 'rb')\n",
    "        next(f)  # skip first line!\n",
    "\n",
    "        annotations = []\n",
    "        for line in f:\n",
    "            annotations.append(line)\n",
    "        f.close\n",
    "        # 3. Preprocessing signal!\n",
    "        if do_preprocess:\n",
    "            # scipy.signal\n",
    "            # median_filter1D\n",
    "            baseline = medfilt(MLII, 71)\n",
    "            baseline = medfilt(baseline, 215)\n",
    "\n",
    "            # Remove Baseline\n",
    "            for i in range(0, len(MLII)):\n",
    "                MLII[i] = MLII[i] - baseline[i]\n",
    "\n",
    "            # TODO Remove High Freqs\n",
    "\n",
    "            # median_filter1D\n",
    "            baseline = medfilt(V1, 71)\n",
    "            baseline = medfilt(baseline, 215)\n",
    "\n",
    "            # Remove Baseline\n",
    "            for i in range(0, len(V1)):\n",
    "                V1[i] = V1[i] - baseline[i]\n",
    "\n",
    "        # Extract the R-peaks from annotations\n",
    "        for a in annotations:\n",
    "            aS = a.split()\n",
    "\n",
    "            pos = int(aS[1])\n",
    "            originalPos = int(aS[1])\n",
    "            classAnttd = aS[2]\n",
    "            if pos > size_RR_max and pos < (len(MLII) - size_RR_max):\n",
    "                index, value = max(enumerate(MLII[pos - size_RR_max: pos + size_RR_max]), key=operator.itemgetter(1))\n",
    "                pos = (pos - size_RR_max) + index\n",
    "\n",
    "            peak_type = 0\n",
    "            # pos = pos-1\n",
    "\n",
    "            if classAnttd in MITBIH_classes:\n",
    "                if (pos > winL and pos < (len(MLII) - winR)):\n",
    "                    beat[r].append((MLII[pos - winL: pos + winR], V1[pos - winL: pos + winR]))\n",
    "                    for i in range(0, len(AAMI_classes)):\n",
    "                        if classAnttd in AAMI_classes[i]:\n",
    "                            class_AAMI = i\n",
    "                            break  # exit loop\n",
    "                    # convert class\n",
    "                    class_ID[r].append(class_AAMI)\n",
    "\n",
    "                    valid_R[r] = np.append(valid_R[r], 1)\n",
    "                else:\n",
    "                    valid_R[r] = np.append(valid_R[r], 0)\n",
    "            else:\n",
    "                valid_R[r] = np.append(valid_R[r], 0)\n",
    "\n",
    "            R_poses[r] = np.append(R_poses[r], pos)\n",
    "            Original_R_poses[r] = np.append(Original_R_poses[r], originalPos)\n",
    "\n",
    "        # R_poses[r] = R_poses[r][(valid_R[r] == 1)]\n",
    "        # Original_R_poses[r] = Original_R_poses[r][(valid_R[r] == 1)]\n",
    "\n",
    "    # Set the data into a bigger struct that keep all the records!\n",
    "    my_db.filename = fRecords\n",
    "\n",
    "    my_db.raw_signal = RAW_signals\n",
    "    my_db.beat = beat  # record, beat, lead\n",
    "    my_db.class_ID = class_ID\n",
    "    my_db.valid_R = valid_R\n",
    "    my_db.R_pos = R_poses\n",
    "    my_db.orig_R_pos = Original_R_poses\n",
    "\n",
    "    return my_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mit_db(DS, winL, winR, do_preprocess, maxRR, use_RR, norm_RR, compute_morph, \n",
    "                db_path, reduced_DS, leads_flag):\n",
    "    print 'Runing train_SVM.py!'\n",
    "\n",
    "    features_labels_name = create_features_labels_name(DS, winL, winR, do_preprocess, maxRR, use_RR, norm_RR,\n",
    "                                                       compute_morph, db_path, reduced_DS, leads_flag)\n",
    "\n",
    "    print features_labels_name\n",
    "    \n",
    "    if os.path.isfile(features_labels_name):\n",
    "        print 'Loading pickle: ' + features_labels_name + '...'\n",
    "        f = open(features_labels_name, 'rb')\n",
    "        # disable garbage collector\n",
    "        gc.disable()  # this improve the required loading time!\n",
    "        features, labels, patient_num_beats = pickle.load(f)\n",
    "        gc.enable()\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    else:\n",
    "        print \"Loading MIT BIH arr (\" + DS + \") ...\"\n",
    "        # 102 and 104 do not have the MLII lead data and 114 have the data on second coloumn\n",
    "        # ML-II\n",
    "        if not reduced_DS:\n",
    "            DS1 = [101, 106, 108, 109, 112, 114, 115, 116, 118, 119, 122, 124, 201, 203, 205, 207, 208, 209, 215, 220,\n",
    "                   223, 230]\n",
    "            DS2 = [100, 103, 105, 111, 113, 117, 121, 123, 200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232,\n",
    "                   233, 234]\n",
    "\n",
    "        # ML-II + V1\n",
    "        else:\n",
    "            DS1 = [101, 106, 108, 109, 112, 115, 118, 119, 201, 203, 205, 207, 208, 209, 215, 220, 223, 230]\n",
    "            DS2 = [105, 111, 113, 121, 200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233, 234]\n",
    "\n",
    "        mit_pickle_name = db_path + '/python_mit'\n",
    "\n",
    "        if reduced_DS:\n",
    "            mit_pickle_name = mit_pickle_name + '_reduced_'\n",
    "\n",
    "        if do_preprocess:\n",
    "            mit_pickle_name = mit_pickle_name + '_rm_bsline'\n",
    "\n",
    "        mit_pickle_name = mit_pickle_name + '_wL_' + str(winL) + '_wR_' + str(winR) + '_' + DS + '.p'\n",
    "\n",
    "        print mit_pickle_name\n",
    "\n",
    "        # If the data with that configuration has been already computed Load pickle\n",
    "        if os.path.isfile(mit_pickle_name):\n",
    "            f = open(mit_pickle_name, 'rb')\n",
    "            # disable garbage collector\n",
    "            gc.disable()  # this improve the required loading time!\n",
    "            my_db = pickle.load(f)\n",
    "            gc.enable()\n",
    "            f.close()\n",
    "        else:  # Load data and compute de RR features\n",
    "            if DS == 'DS1':\n",
    "                my_db = load_signal(DS1, winL, winR, do_preprocess)\n",
    "            else:\n",
    "                my_db = load_signal(DS2, winL, winR, do_preprocess)\n",
    "\n",
    "            print(\"Saving signal processed data ...\")\n",
    "            # Save data\n",
    "            # Protocol version 0 itr_features_balanceds the original ASCII protocol and is backwards compatible with earlier versions of Python.\n",
    "            # Protocol version 1 is the old binary format which is also compatible with earlier versions of Python.\n",
    "            # Protocol version 2 was introduced in Python 2.3. It provides much more efficient pickling of new-style classes.\n",
    "            f = open(mit_pickle_name, 'wb')\n",
    "            pickle.dump(my_db, f, 2)\n",
    "            f.close\n",
    "\n",
    "        features = np.array([], dtype=float)\n",
    "        labels = np.array([], dtype=np.int32)\n",
    "\n",
    "        # This array contains the number of beats for each patient (for cross_val)\n",
    "        patient_num_beats = np.array([], dtype=np.int32)\n",
    "        for p in range(len(my_db.beat)):\n",
    "            patient_num_beats = np.append(patient_num_beats, len(my_db.beat[p]))\n",
    "\n",
    "        \n",
    "    return features, labels, patient_num_beats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deepankar/PhD/Classification\n",
      "Runing train_SVM.py!\n",
      "/home/deepankar/PhD/Classification/features/w_90_90_DS1_rm_bsline_maxRR_RR_norm_RR__MLII.p\n",
      "Loading MIT BIH arr (DS1) ...\n",
      "/home/deepankar/PhD/Classification/python_mit_rm_bsline_wL_90_wR_90_DS1.p\n",
      "/home/deepankar/PhD/Classification\n",
      "Processing signal 0 / 22...\n",
      "/home/deepankar/PhD/Classification/data/csv/101.csv\n",
      "/home/deepankar/PhD/Classification/data/csv/101annotations.txt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'medfilt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-131fd791497c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m [tr_features, tr_labels, tr_patient_num_beats] = load_mit_db('DS1', winL, winR, do_preprocess,\n\u001b[1;32m     23\u001b[0m                                                                  \u001b[0mmaxRR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_RR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_RR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_morph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                                                  db_path, reduced_DS, leads_flag)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-0c4b58979f1c>\u001b[0m in \u001b[0;36mload_mit_db\u001b[0;34m(DS, winL, winR, do_preprocess, maxRR, use_RR, norm_RR, compute_morph, db_path, reduced_DS, leads_flag)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Load data and compute de RR features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDS\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DS1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mmy_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDS1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_preprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mmy_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDS2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwinR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_preprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5322f88220f0>\u001b[0m in \u001b[0;36mload_signal\u001b[0;34m(DS, winL, winR, do_preprocess)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# scipy.signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# median_filter1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedfilt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLII\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m71\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedfilt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m215\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'medfilt' is not defined"
     ]
    }
   ],
   "source": [
    "winL=90\n",
    "winR=90\n",
    "do_preprocess=True\n",
    "use_weight_class=True\n",
    "maxRR=True\n",
    "use_RR=True\n",
    "norm_RR=True\n",
    "compute_morph={''}\n",
    "oversamp_method = ''\n",
    "pca_k = ''\n",
    "feature_selection = ''\n",
    "do_cross_val = ''\n",
    "C_value = 0.001\n",
    "gamma_value = 0.0\n",
    "reduced_DS = False\n",
    "leads_flag = [1,0]\n",
    "db_path = os.getcwd()\n",
    "print db_path\n",
    "DS1 = [101, 106, 108, 109, 112, 114, 115, 116, 118, 119, 122, 124, 201, 203, 205, 207, 208, 209, 215, 220,\n",
    "                   223, 230]\n",
    "\n",
    "[tr_features, tr_labels, tr_patient_num_beats] = load_mit_db('DS1', winL, winR, do_preprocess,\n",
    "                                                                 maxRR, use_RR, norm_RR, compute_morph,\n",
    "                                                                 db_path, reduced_DS, leads_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deepankar/PhD/Classification\n"
     ]
    }
   ],
   "source": [
    "DS=DS1\n",
    "winL=90\n",
    "winR=90\n",
    "do_preprocess=True\n",
    "class_ID = [[] for i in range(len(DS))]\n",
    "beat = [[] for i in range(len(DS))]  # record, beat, lead\n",
    "R_poses = [np.array([]) for i in range(len(DS))]\n",
    "Original_R_poses = [np.array([]) for i in range(len(DS))]\n",
    "valid_R = [np.array([]) for i in range(len(DS))]\n",
    "my_db = mit_db()\n",
    "size_RR_max = 20\n",
    "pathDB = os.getcwd()\n",
    "print pathDB\n",
    "DB_name = 'data'\n",
    "# Read files: signal (.csv )  annotations (.txt)\n",
    "fRecords = list()\n",
    "fAnnotations = list()\n",
    "lst = os.listdir(pathDB + \"/\" + DB_name + \"/csv\")\n",
    "lst.sort()\n",
    "for filename in lst:\n",
    "    if filename.endswith(\".csv\"):\n",
    "        if int(filename[0:3]) in DS:\n",
    "            fRecords.append(filename)\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        if int(filename[0:3]) in DS:\n",
    "            fAnnotations.append(filename)\n",
    "    \n",
    "# WE ARE CURENTLY USING ALL THE CLASSES\n",
    "MITBIH_classes = ['N', 'L', 'R', 'e', 'j', 'A', 'a', 'J', 'S', 'V', 'E', 'F', 'P', '/', 'f', 'u']\n",
    "AAMI_classes = [['N', 'L', 'R'], ['A', 'a', 'J', 'S', 'e', 'j'], ['V', 'E'], ['F'], ['P', '/', 'f', 'u']]\n",
    "\n",
    "RAW_signals = []\n",
    "filename = pathDB + \"/\" + DB_name + \"/csv/\" + fRecords[0]\n",
    "df = pd.read_csv(filename)\n",
    "MLII = df['\\'MLII\\''].values\n",
    "V1 = df['\\'V1\\''].values\n",
    "RAW_signals.append((MLII, V1))  ## NOTE a copy must be created in order to preserve the original signal\n",
    "# display_signal(MLII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "\n",
    "baseline = medfilt(MLII, 71)\n",
    "baseline = medfilt(baseline, 215)\n",
    "\n",
    "MLII = MLII-baseline\n",
    "MLII\n",
    "\n",
    "baseline = medfilt(V1, 71)\n",
    "baseline = medfilt(baseline, 215)\n",
    "V1 = V1-baseline\n",
    "V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deepankar/PhD/Classification/data/csv/101annotations.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function close>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Read annotations\n",
    "filename = pathDB + \"/\" + DB_name + \"/csv/\" + fAnnotations[0]\n",
    "print filename\n",
    "f = open(filename, 'rb')\n",
    "next(f)  # skip first line!\n",
    "annotations = []\n",
    "for line in f:\n",
    "    annotations.append(line)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deepankar/PhD/Classification/data/csv/101annotations.txt\n"
     ]
    }
   ],
   "source": [
    "filename = pathDB + \"/\" + DB_name + \"/csv/\" + fAnnotations[0]\n",
    "print filename\n",
    "# data = pd.read_csv(filename, sep=\" \",names=['Time','Sample','#','Type','Sub','Chan','Num','Aux'])\n",
    "data = pd.read_csv(filename, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.values[501][0]\n",
    "aa = a.split()\n",
    "\n",
    "pos = int(aa[1])\n",
    "originalpos = int(aa[1])\n",
    "classAnttd = aa[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 365.0 161716 161717\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "if pos > size_RR_max and pos < (len(MLII) - size_RR_max):\n",
    "    value = max(MLII[pos - size_RR_max: pos + size_RR_max])\n",
    "    index = np.argmax(MLII[pos - size_RR_max: pos + size_RR_max])\n",
    "    pos = (pos - size_RR_max) + index\n",
    "    print index, value, pos, int(aa[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if classAnttd in MITBIH_classes:\n",
    "    if (pos > winL and pos < (len(MLII) - winR)):\n",
    "        beat[r].append((MLII[pos - winL: pos + winR], V1[pos - winL: pos + winR]))\n",
    "        for i in range(0, len(AAMI_classes)):\n",
    "            if classAnttd in AAMI_classes[i]:\n",
    "                class_AAMI = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(AAMI_classes)):\n",
    "    if classAnttd in AAMI_classes[i]:\n",
    "        print classAnttd,i\n",
    "#         print AAMI_classes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if classAnttd in MITBIH_classes:\n",
    "    if (pos > winL and pos < (len(MLII) - winR)):\n",
    "        beat[r].append((MLII[pos - winL: pos + winR], V1[pos - winL: pos + winR]))\n",
    "        for i in range(0, len(AAMI_classes)):\n",
    "            if classAnttd in AAMI_classes[i]:\n",
    "                class_AAMI = i\n",
    "                break  # exit loop\n",
    "        # convert class\n",
    "        class_ID[r].append(class_AAMI)\n",
    "\n",
    "        valid_R[r] = np.append(valid_R[r], 1)\n",
    "    else:\n",
    "        valid_R[r] = np.append(valid_R[r], 0)\n",
    "else:\n",
    "    valid_R[r] = np.append(valid_R[r], 0)\n",
    "\n",
    "R_poses[r] = np.append(R_poses[r], pos)\n",
    "Original_R_poses[r] = np.append(Original_R_poses[r], originalPos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Extract the R-peaks from annotations\n",
    "    for a in annotations:\n",
    "        aS = a.split()\n",
    "\n",
    "        pos = int(aS[1])\n",
    "        originalPos = int(aS[1])\n",
    "        classAnttd = aS[2]\n",
    "        if pos > size_RR_max and pos < (len(MLII) - size_RR_max):\n",
    "            index, value = max(enumerate(MLII[pos - size_RR_max: pos + size_RR_max]), key=operator.itemgetter(1))\n",
    "            pos = (pos - size_RR_max) + index\n",
    "\n",
    "        peak_type = 0\n",
    "        # pos = pos-1\n",
    "\n",
    "        if classAnttd in MITBIH_classes:\n",
    "            if (pos > winL and pos < (len(MLII) - winR)):\n",
    "                beat[r].append((MLII[pos - winL: pos + winR], V1[pos - winL: pos + winR]))\n",
    "                for i in range(0, len(AAMI_classes)):\n",
    "                    if classAnttd in AAMI_classes[i]:\n",
    "                        class_AAMI = i\n",
    "                        break  # exit loop\n",
    "                # convert class\n",
    "                class_ID[r].append(class_AAMI)\n",
    "\n",
    "                valid_R[r] = np.append(valid_R[r], 1)\n",
    "            else:\n",
    "                valid_R[r] = np.append(valid_R[r], 0)\n",
    "        else:\n",
    "            valid_R[r] = np.append(valid_R[r], 0)\n",
    "\n",
    "        R_poses[r] = np.append(R_poses[r], pos)\n",
    "        Original_R_poses[r] = np.append(Original_R_poses[r], originalPos)\n",
    "\n",
    "    # R_poses[r] = R_poses[r][(valid_R[r] == 1)]\n",
    "    # Original_R_poses[r] = Original_R_poses[r][(valid_R[r] == 1)]\n",
    "\n",
    "# Set the data into a bigger struct that keep all the records!\n",
    "my_db.filename = fRecords\n",
    "\n",
    "my_db.raw_signal = RAW_signals\n",
    "my_db.beat = beat  # record, beat, lead\n",
    "my_db.class_ID = class_ID\n",
    "my_db.valid_R = valid_R\n",
    "my_db.R_pos = R_poses\n",
    "my_db.orig_R_pos = Original_R_poses\n",
    "\n",
    "return my_db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
